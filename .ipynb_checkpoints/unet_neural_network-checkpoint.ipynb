{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6632304-091f-403b-931a-d500009581e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(profile=\"full\", linewidth=200, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6368f4-8271-45bd-9ade-de9d5371cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
    "    \"\"\"\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "    \n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = timesteps.type(torch.float32)[:, None] * emb[None, :]\n",
    "    emb = torch.concat([torch.sin(emb), torch.cos(emb)], axis=1)\n",
    "\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "      emb = torch.pad(emb, [[0, 0], [0, 1]])\n",
    "      \n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim), f\"{emb.shape}\"\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1b6579-0d26-439e-99e8-130b7e73b104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (torch.rand(100)*10).long()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde50fc3-58d2-4f87-81f3-4980a50bb0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timestep_embedding(t, 64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe27d82c-ff85-45e8-8241-c8a9f3a5c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        \"\"\"\n",
    "        param C:input and output channels\n",
    "        \"\"\"\n",
    "        super(Downsample, self).__init__()\n",
    "        self.C = C\n",
    "        self.conv = nn.Conv2d(in_channels=C, out_channels=C, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x =  self.conv(x)\n",
    "        assert x.shape == (B, C, H // 2, W // 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a675342-8ce5-401b-a395-0dba12e03a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 200, 200])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (torch.rand(100)*10).long()\n",
    "emb = get_timestep_embedding(t, 64)\n",
    "print(emb.shape)\n",
    "\n",
    "model = Downsample(64)\n",
    "img = torch.randn((10, 64, 400, 400))\n",
    "out = model(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f953c331-eb0c-4ced-8447-95283bd83aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        \"\"\"\n",
    "        param C:input and output channels\n",
    "        \"\"\"\n",
    "        super(Upsample, self).__init__()\n",
    "        self.C = C\n",
    "        self.conv = nn.Conv2d(in_channels=C, out_channels=C, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = nn.functional.interpolate(x, size=None, scale_factor=2, mode='nearest-exact')\n",
    "        assert x.shape == (B, C, H * 2, W * 2)\n",
    "        x =  self.conv(x)\n",
    "        assert x.shape == (B, C, H * 2, W * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690925db-2a9f-4d5e-bd03-649b965076c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64])\n",
      "torch.Size([10, 64, 200, 200])\n",
      "torch.Size([10, 64, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "t = (torch.rand(100)*10).long()\n",
    "emb = get_timestep_embedding(t, 64)\n",
    "print(emb.shape)\n",
    "\n",
    "downsample = Downsample(64)\n",
    "img = torch.randn((10, 64, 400, 400))\n",
    "h = downsample(img)\n",
    "print(h.shape)\n",
    "\n",
    "upsample = Upsample(64)\n",
    "out = upsample(h)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffeb622-bc69-4dc3-8860-e617639adaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nin(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic block of ResNet\n",
    "    it is like applying MLP to a 2D image and modifying the number of channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, scale = 1e-10):\n",
    "        super(Nin, self).__init__()\n",
    "        n= (in_dim + out_dim) / 2\n",
    "        limit = np.sqrt(3 * scale / n)\n",
    "        self.W = torch.nn.Parameter(torch.zeros((in_dim, out_dim), dtype= torch.float32\n",
    "                                              ).uniform_(-limit, limit))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((1, out_dim, 1 , 1), dtype= torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.einsum(\"bchw, co->bowh\", x, self.W) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9516b8-6cf7-4cc7-8cc7-a0cdc480a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64])\n",
      "torch.Size([10, 64, 200, 200])\n",
      "torch.Size([10, 64, 400, 400])\n",
      "torch.Size([10, 128, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "t = (torch.rand(100)*10).long()\n",
    "emb = get_timestep_embedding(t, 64)\n",
    "print(emb.shape)\n",
    "\n",
    "downsample = Downsample(64)\n",
    "img = torch.randn((10, 64, 400, 400))\n",
    "h = downsample(img)\n",
    "print(h.shape)\n",
    "\n",
    "upsample = Upsample(64)\n",
    "img = upsample(h)\n",
    "print(img.shape)\n",
    "\n",
    "nin = Nin(64, 128)\n",
    "print(nin(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69694f8-d5bb-49b2-a536-8647301bd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.dense = nn.Linear(512, out_ch)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # needed to do the skip connection\n",
    "        if not (in_ch == out_ch):\n",
    "            self.nin = Nin(in_ch, out_ch)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, temb): # temb [batch_size, 512] -> B, out_ch\n",
    "        \"\"\"\n",
    "        param x:    (B, C, H, W)\n",
    "        param temb: (B, dim)\n",
    "                \n",
    "        \"\"\"\n",
    "        print(f\"\\nx shape: {x.shape}, temb shape: {temb.shape}\")\n",
    "        #print(f\"{}\")\n",
    "        h = self.nonlinearity(nn.functional.group_norm(x, num_groups=32))\n",
    "        h= self.conv1(h)\n",
    "        print(f\"after conv1: {h.shape}\")\n",
    "        \n",
    "        # add in timestep embedding\n",
    "        h +=  self.dense(self.nonlinearity(temb))[:, :, None, None]\n",
    "        h = self.nonlinearity(nn.functional.group_norm(h, num_groups=32))\n",
    "        \n",
    "        h= nn.functional.dropout(h, p=self.dropout_rate)\n",
    "        print(f\"after non linearity: {self.dense(self.nonlinearity(temb))[:, :, None, None].shape}\")\n",
    "        h = self.conv2(h)\n",
    "        print(f\"after conv2: {h.shape}\")\n",
    "\n",
    "        if not (x.shape[1] == h.shape[1]):\n",
    "            print(\"reshaping x\")\n",
    "            x = self.nin(x)\n",
    "            \n",
    "        print(f\"before output x shape: {x.shape}, temb shape: {h.shape}\")\n",
    "        assert x.shape == h.shape\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe466fa-c6b8-41b8-93d8-97b20a4bea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t shape: torch.Size([10])\n",
      "embedded t shape: torch.Size([10, 512])\n",
      "downsampled image shape: torch.Size([10, 64, 64, 64])\n",
      "upsampled image shape: torch.Size([10, 64, 128, 128])\n",
      "after Nin application shape: torch.Size([10, 128, 128, 128])\n",
      "\n",
      "x shape: torch.Size([10, 128, 128, 128]), temb shape: torch.Size([10, 512])\n",
      "after conv1: torch.Size([10, 128, 128, 128])\n",
      "after non linearity: torch.Size([10, 128, 1, 1])\n",
      "after conv2: torch.Size([10, 128, 128, 128])\n",
      "before output x shape: torch.Size([10, 128, 128, 128]), temb shape: torch.Size([10, 128, 128, 128])\n",
      "final x + h shape: torch.Size([10, 128, 128, 128])\n",
      "\n",
      "x shape: torch.Size([10, 128, 128, 128]), temb shape: torch.Size([10, 512])\n",
      "after conv1: torch.Size([10, 64, 128, 128])\n",
      "after non linearity: torch.Size([10, 64, 1, 1])\n",
      "after conv2: torch.Size([10, 64, 128, 128])\n",
      "reshaping x\n",
      "before output x shape: torch.Size([10, 64, 128, 128]), temb shape: torch.Size([10, 64, 128, 128])\n",
      "final x + h shape: torch.Size([10, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "t = (torch.rand(10)*10).long()\n",
    "print(f\"t shape: {t.shape}\")\n",
    "temb = get_timestep_embedding(t, 512)\n",
    "print(f\"embedded t shape: {temb.shape}\")\n",
    "\n",
    "downsample = Downsample(64)\n",
    "img = torch.randn((10, 64, 128, 128))\n",
    "h = downsample(img)\n",
    "print(f\"downsampled image shape: {h.shape}\")\n",
    "\n",
    "upsample = Upsample(64)\n",
    "img = upsample(h)\n",
    "print(f\"upsampled image shape: {img.shape}\")\n",
    "\n",
    "nin = Nin(64, 128)\n",
    "img = nin(img)\n",
    "print(f\"after Nin application shape: {img.shape}\")\n",
    "\n",
    "resnet = ResNetBlock(128, 128, 0.1)\n",
    "img = resnet(img, temb)\n",
    "print(f\"final x + h shape: {img.shape}\")\n",
    "\n",
    "resnet = ResNetBlock(128, 64, 0.1)\n",
    "img = resnet(img, temb)\n",
    "print(f\"final x + h shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6fb4df-43a7-4a1b-92ab-6d84a0663770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.Q = Nin(ch, ch)\n",
    "        self.K = Nin(ch, ch)\n",
    "        self.V = Nin(ch, ch)\n",
    "        self.ch = ch\n",
    "        self.nin = Nin(ch, ch, scale = 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert C == self.ch\n",
    "\n",
    "        h = nn.functional.group_norm(x, num_groups=32)\n",
    "        q = self.Q(h)\n",
    "        k = self.K(h)\n",
    "        v = self.V(h)\n",
    "\n",
    "        w = torch.einsum('bchw,bcHW->bhwHW', q, k) * (int(C) ** (-0.5)) # [B, H, W, H, W]\n",
    "        w = torch.reshape(w, [B, H, W, H * W])\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = torch.reshape(w, [B, H, W, H, W])\n",
    "\n",
    "        h = torch.einsum('bhwHW,bcHW->bchw', w, v)\n",
    "        h = self.nin(h)\n",
    "\n",
    "        assert h.shape == x.shape\n",
    "        \n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2b7220-331a-4667-a4bc-e88c72c50bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t shape: torch.Size([10])\n",
      "embedded t shape: torch.Size([10, 512])\n",
      "downsampled image shape: torch.Size([10, 64, 8, 8])\n",
      "upsampled image shape: torch.Size([10, 64, 16, 16])\n",
      "after Nin application shape: torch.Size([10, 128, 16, 16])\n",
      "\n",
      "x shape: torch.Size([10, 128, 16, 16]), temb shape: torch.Size([10, 512])\n",
      "after conv1: torch.Size([10, 128, 16, 16])\n",
      "after non linearity: torch.Size([10, 128, 1, 1])\n",
      "after conv2: torch.Size([10, 128, 16, 16])\n",
      "before output x shape: torch.Size([10, 128, 16, 16]), temb shape: torch.Size([10, 128, 16, 16])\n",
      "final x + h shape: torch.Size([10, 128, 16, 16])\n",
      "\n",
      "x shape: torch.Size([10, 128, 16, 16]), temb shape: torch.Size([10, 512])\n",
      "after conv1: torch.Size([10, 64, 16, 16])\n",
      "after non linearity: torch.Size([10, 64, 1, 1])\n",
      "after conv2: torch.Size([10, 64, 16, 16])\n",
      "reshaping x\n",
      "before output x shape: torch.Size([10, 64, 16, 16]), temb shape: torch.Size([10, 64, 16, 16])\n",
      "final x + h shape: torch.Size([10, 64, 16, 16])\n",
      "attention block output shape: torch.Size([10, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "t = (torch.rand(10)*10).long()\n",
    "print(f\"t shape: {t.shape}\")\n",
    "temb = get_timestep_embedding(t, 512)\n",
    "print(f\"embedded t shape: {temb.shape}\")\n",
    "\n",
    "downsample = Downsample(64)\n",
    "img = torch.randn((10, 64, 16, 16))\n",
    "h = downsample(img)\n",
    "print(f\"downsampled image shape: {h.shape}\")\n",
    "\n",
    "upsample = Upsample(64)\n",
    "img = upsample(h)\n",
    "print(f\"upsampled image shape: {img.shape}\")\n",
    "\n",
    "nin = Nin(64, 128)\n",
    "img = nin(img)\n",
    "print(f\"after Nin application shape: {img.shape}\")\n",
    "\n",
    "resnet = ResNetBlock(128, 128, 0.1)\n",
    "img = resnet(img, temb)\n",
    "print(f\"final x + h shape: {img.shape}\")\n",
    "\n",
    "resnet = ResNetBlock(128, 64, 0.1)\n",
    "img = resnet(img, temb)\n",
    "print(f\"final x + h shape: {img.shape}\")\n",
    "\n",
    "\n",
    "att = AttentionBlock(64)\n",
    "img = att(img)\n",
    "print(f\"attention block output shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc68e3-da51-408d-a2a7-f29aa34aad6f",
   "metadata": {},
   "source": [
    "### UNet Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c890fe14-dcec-4b1c-9f90-b3e4c47747b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.dense = nn.Linear(512, out_ch)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # needed to do the skip connection\n",
    "        if not (in_ch == out_ch):\n",
    "            self.nin = Nin(in_ch, out_ch)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, temb): # temb [batch_size, 512] -> B, out_ch\n",
    "        \"\"\"\n",
    "        param x:    (B, C, H, W)\n",
    "        param temb: (B, dim)\n",
    "                \n",
    "        \"\"\"\n",
    "        h = self.nonlinearity(nn.functional.group_norm(x, num_groups=32))\n",
    "        h= self.conv1(h)\n",
    "        # add in timestep embedding\n",
    "        h +=  self.dense(self.nonlinearity(temb))[:, :, None, None]\n",
    "        h = self.nonlinearity(nn.functional.group_norm(h, num_groups=32))\n",
    "        h= nn.functional.dropout(h, p=self.dropout_rate)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "\n",
    "        if not (x.shape[1] == h.shape[1]):\n",
    "            x = self.nin(x)\n",
    "\n",
    "        assert x.shape == h.shape\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "930eb493-a135-4c78-9810-f92db92ad654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, ch=128, in_ch=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.ch = ch\n",
    "        self.in_ch = in_ch\n",
    "        self.linear1 = nn.Linear(ch, 4 * ch)\n",
    "        self.linear2 = nn.Linear(ch * 4, ch * 4)\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_ch, ch, 3, stride=1, padding=1)\n",
    "        self.down = nn.ModuleList([ ResNetBlock(ch, 1 * ch), # [32, 32] # first block\n",
    "                                    ResNetBlock(1 * ch, 1 * ch),  # first block\n",
    "                                    Downsample(1 * ch), # [16, 16] # first block\n",
    "                                   \n",
    "                                    ResNetBlock(1 * ch, 2 * ch), # second block\n",
    "                                    AttentionBlock(2 * ch), # second block\n",
    "                                    ResNetBlock(2 * ch, 2 * ch), # second block\n",
    "                                    AttentionBlock(2 * ch), # second block\n",
    "                                    Downsample(2 * ch), # [16, 16] # second block\n",
    "\n",
    "                                    ResNetBlock(2 * ch, 2 * ch), # third block\n",
    "                                    ResNetBlock(2 * ch, 2 * ch), # third block\n",
    "                                    Downsample(2 * ch), # [16, 16] # third block\n",
    "\n",
    "                                    ResNetBlock(2 * ch, 2 * ch), # forth block\n",
    "                                    ResNetBlock(2 * ch, 2 * ch), # forth block\n",
    "        ])\n",
    "        \n",
    "        self.middle = nn.ModuleList([ResNetBlock(2 * ch, 2 * ch),\n",
    "                                     AttentionBlock(2 * ch),\n",
    "                                     ResNetBlock(2 * ch, 2 * ch)\n",
    "        ])\n",
    "\n",
    "        self.up = nn.ModuleList([ResNetBlock(4 * ch, 2 * ch), # first block [4, 4]\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # first block\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # first block\n",
    "                                 Upsample(2 * ch), # first block [8, 8]\n",
    "\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # second block [8, 8]\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # second block\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # second block\n",
    "                                 Upsample(2 * ch), # second block [16, 16]\n",
    "\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # third block [16, 16]\n",
    "                                 AttentionBlock(2 * ch),  # third block\n",
    "                                 ResNetBlock(4 * ch, 2 * ch), # third block\n",
    "                                 AttentionBlock(2 * ch),  # third block\n",
    "                                 ResNetBlock(3 * ch, 2 * ch), # third block\n",
    "                                 AttentionBlock(2 * ch),  # third block\n",
    "                                 Upsample(2 * ch), # third block\n",
    "                                 \n",
    "                                 ResNetBlock(3 * ch, ch), # forth block\n",
    "                                 ResNetBlock(2 * ch, ch), # forth block\n",
    "                                 ResNetBlock(2 * ch, ch), # forth block     \n",
    "        ])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(ch, in_ch, 3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        param x (torch.Tensor): batch of of images [B, C, H, W]\n",
    "        param t (torch.Tensor): tensor of time steps (torch.long) [B]\n",
    "        \"\"\"\n",
    "        temb = get_timestep_embedding(t, self.ch)\n",
    "        temb = torch.nn.functional.silu(self.linear1(temb))\n",
    "        temb = self.linear2(temb)\n",
    "        assert temb.shape == (t.shape[0], self.ch*4)\n",
    "        print(f\"\\ntemb after tansformation shape: {temb.shape}\")\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        print(f\"x1 after conv1 shape: {x1.shape}\")\n",
    "\n",
    "        # DownSampling\n",
    "        x2  = self.down[0](x1, temb)\n",
    "        x3  = self.down[1](x2, temb)\n",
    "        x4  = self.down[2](x3)\n",
    "        x5  = self.down[3](x4, temb)\n",
    "        x6  = self.down[4](x5) # Attention\n",
    "        x7  = self.down[5](x6, temb)\n",
    "        x8  = self.down[6](x7) # Attention\n",
    "        x9  = self.down[7](x8)\n",
    "        x10 = self.down[8](x9, temb)\n",
    "        x11 = self.down[9](x10, temb)\n",
    "        x12 = self.down[10](x11)\n",
    "        x13 = self.down[11](x12, temb)\n",
    "        x14 = self.down[12](x13, temb)\n",
    "        print(f\"output from Downsampling block {x14.shape}\")\n",
    "\n",
    "        # Middle\n",
    "        x = self.middle[0](x14, temb)\n",
    "        x = self.middle[1](x)\n",
    "        x = self.middle[2](x, temb)\n",
    "        print(f\"output from Middle block {x.shape}\")\n",
    "\n",
    "        # UpSampling\n",
    "        x = self.up[0](torch.cat((x, x14), dim=1), temb)\n",
    "        x = self.up[1](torch.cat((x, x13), dim=1), temb)\n",
    "        x = self.up[2](torch.cat((x, x12), dim=1), temb)\n",
    "        x = self.up[3](x)\n",
    "        x = self.up[4](torch.cat((x, x11), dim=1), temb)\n",
    "        x = self.up[5](torch.cat((x, x10), dim=1), temb)\n",
    "        x = self.up[6](torch.cat((x, x9), dim=1), temb)\n",
    "        x = self.up[7](x)\n",
    "        x = self.up[8](torch.cat((x, x8), dim=1), temb)\n",
    "        x = self.up[9](x)\n",
    "        x = self.up[10](torch.cat((x, x6), dim=1), temb)\n",
    "        x = self.up[11](x)\n",
    "        x = self.up[12](torch.cat((x, x4), dim=1), temb)\n",
    "        x = self.up[13](x)\n",
    "        x = self.up[14](x)\n",
    "        x = self.up[15](torch.cat((x, x3), dim=1), temb)\n",
    "        x = self.up[16](torch.cat((x, x2), dim=1), temb)\n",
    "        x = self.up[17](torch.cat((x, x1), dim=1), temb)\n",
    "\n",
    "        x = torch.nn.functional.silu(nn.functional.group_norm(x, num_groups=32))\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ebeb96-2e53-42c3-a252-382e8da667eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image input shape to UNet torch.Size([10, 1, 32, 32])\n",
      "\n",
      "temb after tansformation shape: torch.Size([10, 512])\n",
      "x1 after conv1 shape: torch.Size([10, 128, 32, 32])\n",
      "output from Downsampling block torch.Size([10, 256, 4, 4])\n",
      "output from Middle block torch.Size([10, 256, 4, 4])\n",
      "image output shape from UNet torch.Size([10, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn((10, 1, 32, 32))\n",
    "print(f\"image input shape to UNet {img.shape}\")\n",
    "unet =UNet(in_ch=1)\n",
    "out = unet(img, t)\n",
    "print(f\"image output shape from UNet {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79902f48-bd0d-4221-9dfd-d0321f9267e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.713281"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in unet.parameters()]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2e900-e316-42c9-8541-b900a714d092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
